---
title: canal部署
date: 2021-09-08 17:09:12
permalink: /pages/0750d6/
categories: 
  - linux
  - Linux
tags: 
  - canal
  - linux
---
# canal部署

canal官方仓库:`https://github.com/alibaba/canal/`

wiki:`https://github.com/alibaba/canal/wiki`

canal的用途是基于mysql的binlog日志解析，提供增量数据的订阅和消费。简单的场景：通过canal监控mysql数据变更从而及时更新redis中对应的缓存。

本文主要介绍canal在服务器端的部署，包括`canal-admin`,`canal-tsdb配置`以及`instance`配置。

首先需要下载canal的发行版，下载地址：https://github.com/alibaba/canal/releases，可自行选择版本，这里直接选择最新的v1.1.5。系统版本为centos7。

## canal admin

- 解压

```bash
tar zxvf canal.admin-1.1.5.tar.gz -C /home/install/canal/admin
```

- 执行conf文件夹中的canal_manager.sql建表

  ![image-20210617000438523](https://io.storyxc.com/image-20210617000438523.png)

  修改conf文件夹中的application.yml

  ![image-20210617001123953](https://io.storyxc.com/image-20210617001123953.png)

  - 开放admin控制台的端口，以默认的8089为例

    开放端口：`firewall-cmd --zone=public --add-port=8089/tcp --permanent`

    配置立即生效：`firewall-cmd --reload`

- 执行bin目录的startup.sh

- 访问8089端口

  ![image-20210617001200010](https://io.storyxc.com/image-20210617001200010.png)

  

- 使用默认账号密码 admin/123456即可登录

- ![image-20210617001457272](https://io.storyxc.com/image-20210617001457272.png)



> > canal-admin的核心模型主要有：
>
> 1. instance，对应canal-server里的instance，一个最小的订阅mysql的队列
> 2. server，对应canal-server，一个server里可以包含多个instance
> 3. 集群，对应一组canal-server，组合在一起面向高可用HA的运维
>
> 简单解释：
>
> 1. instance因为是最原始的业务订阅诉求，它会和 server/集群 这两个面向资源服务属性的进行关联，比如instance A绑定到server A上或者集群 A上，
> 2. 有了任务和资源的绑定关系后，对应的资源服务就会接收到这个任务配置，在对应的资源上动态加载instance，并提供服务
>    - 动态加载的过程，对应配置文件中的autoScan配置，只不过基于canal-admin之后可就以变为远程的web操作，而不需要在机器上运维配置文件
> 3. 将server抽象成资源之后，原本canal-server运行所需要的canal.properties/instance.properties配置文件就需要在web ui上进行统一运维，每个server只需要以最基本的启动配置 (比如知道一下canal-admin的manager地址，以及访问配置的账号、密码即可)

- 新建server，按照图中配置即可

  ![image-20210617001928077](https://io.storyxc.com/image-20210617001928077.png)

  > 配置项：
  >
  > - 所属集群，可以选择为单机 或者 集群。一般单机Server的模式主要用于一次性的任务或者测试任务
  > - Server名称，唯一即可，方便自己记忆
  > - Server Ip，机器ip
  > - admin端口，canal 1.1.4版本新增的能力，会在canal-server上提供远程管理操作，默认值11110
  > - tcp端口，canal提供netty数据订阅服务的端口
  > - metric端口， promethues的exporter监控数据端口 (未来会对接监控)

- instance运维

  - 创建instance

    ![image-20210617002844737](https://io.storyxc.com/image-20210617002844737.png)

## canal deployer

- 解压

```bash
tar xzvf canal.deployer-1.1.5.tar.gz -C /home/install/canal/deployer
```

- 修改conf目录下的canal.properties

  ```properties
  #################################################
  ######### 		common argument		#############
  #################################################
  # tcp bind ip
  canal.ip =
  # register ip to zookeeper
  canal.register.ip = 127.0.0.1
  canal.port = 11111
  canal.metrics.pull.port = 11112
  # canal instance user/passwd
  # canal.user = canal
  # canal.passwd = E3619321C1A937C46A0D8BD1DAC39F93B27D4458
  
  # canal admin config
  #canal.admin.manager = 127.0.0.1:8089
  canal.admin.port = 11110
  canal.admin.user = admin
  canal.admin.passwd = 4ACFE3202A5FF5CF467898FC58AAB1D615029441
  # admin auto register
  #canal.admin.register.auto = true
  #canal.admin.register.cluster =
  #canal.admin.register.name =
  
  canal.zkServers =
  # flush data to zk
  canal.zookeeper.flush.period = 1000
  canal.withoutNetty = false
  # tcp, kafka, rocketMQ, rabbitMQ
  canal.serverMode = tcp
  # flush meta cursor/parse position to file
  canal.file.data.dir = ${canal.conf.dir}
  canal.file.flush.period = 1000
  ## memory store RingBuffer size, should be Math.pow(2,n)
  canal.instance.memory.buffer.size = 16384
  ## memory store RingBuffer used memory unit size , default 1kb
  canal.instance.memory.buffer.memunit = 1024 
  ## meory store gets mode used MEMSIZE or ITEMSIZE
  canal.instance.memory.batch.mode = MEMSIZE
  canal.instance.memory.rawEntry = true
  
  ## detecing config
  canal.instance.detecting.enable = false
  #canal.instance.detecting.sql = insert into retl.xdual values(1,now()) on duplicate key update x=now()
  canal.instance.detecting.sql = select 1
  canal.instance.detecting.interval.time = 3
  canal.instance.detecting.retry.threshold = 3
  canal.instance.detecting.heartbeatHaEnable = false
  
  # support maximum transaction size, more than the size of the transaction will be cut into multiple transactions delivery
  canal.instance.transaction.size =  1024
  # mysql fallback connected to new master should fallback times
  canal.instance.fallbackIntervalInSeconds = 60
  
  # network config
  canal.instance.network.receiveBufferSize = 16384
  canal.instance.network.sendBufferSize = 16384
  canal.instance.network.soTimeout = 30
  
  # binlog filter config
  canal.instance.filter.druid.ddl = true
  canal.instance.filter.query.dcl = false
  canal.instance.filter.query.dml = false
  canal.instance.filter.query.ddl = false
  canal.instance.filter.table.error = false
  canal.instance.filter.rows = false
  canal.instance.filter.transaction.entry = false
  canal.instance.filter.dml.insert = false
  canal.instance.filter.dml.update = false
  canal.instance.filter.dml.delete = false
  
  # binlog format/image check
  canal.instance.binlog.format = ROW,STATEMENT,MIXED 
  canal.instance.binlog.image = FULL,MINIMAL,NOBLOB
  
  # binlog ddl isolation
  canal.instance.get.ddl.isolation = false
  
  # parallel parser config
  canal.instance.parser.parallel = true
  ## concurrent thread number, default 60% available processors, suggest not to exceed Runtime.getRuntime().availableProcessors()
  #canal.instance.parser.parallelThreadSize = 16
  ## disruptor ringbuffer size, must be power of 2
  canal.instance.parser.parallelBufferSize = 256
  
  # table meta tsdb info
  canal.instance.tsdb.enable = true
  #canal.instance.tsdb.dir = ${canal.file.data.dir:../conf}/${canal.instance.destination:}
  canal.instance.tsdb.url = jdbc:mysql://127.0.0.1:3306/canal_tsdb
  canal.instance.tsdb.dbUsername = root
  canal.instance.tsdb.dbPassword = root
  # dump snapshot interval, default 24 hour
  canal.instance.tsdb.snapshot.interval = 24
  # purge snapshot expire , default 360 hour(15 days)
  canal.instance.tsdb.snapshot.expire = 360
  
  #################################################
  ######### 		destinations		#############
  #################################################
  canal.destinations = instance-A
  # conf root dir
  canal.conf.dir = ../conf
  # auto scan instance dir add/remove and start/stop instance
  canal.auto.scan = true
  canal.auto.scan.interval = 5
  # set this value to 'true' means that when binlog pos not found, skip to latest.
  # WARN: pls keep 'false' in production env, or if you know what you want.
  canal.auto.reset.latest.pos.mode = false
  
  #canal.instance.tsdb.spring.xml = classpath:spring/tsdb/h2-tsdb.xml
  canal.instance.tsdb.spring.xml = classpath:spring/tsdb/mysql-tsdb.xml
  
  canal.instance.global.mode = spring
  canal.instance.global.lazy = false
  canal.instance.global.manager.address = ${canal.admin.manager}
  #canal.instance.global.spring.xml = classpath:spring/memory-instance.xml
  canal.instance.global.spring.xml = classpath:spring/file-instance.xml
  #canal.instance.global.spring.xml = classpath:spring/default-instance.xml
  
  ##################################################
  ######### 	      MQ Properties      #############
  ##################################################
  # aliyun ak/sk , support rds/mq
  canal.aliyun.accessKey =
  canal.aliyun.secretKey =
  canal.aliyun.uid=
  
  canal.mq.flatMessage = true
  canal.mq.canalBatchSize = 50
  canal.mq.canalGetTimeout = 100
  # Set this value to "cloud", if you want open message trace feature in aliyun.
  canal.mq.accessChannel = local
  
  canal.mq.database.hash = true
  canal.mq.send.thread.size = 30
  canal.mq.build.thread.size = 8
  
  ##################################################
  ######### 		     Kafka 		     #############
  ##################################################
  kafka.bootstrap.servers = 127.0.0.1:9092
  kafka.acks = all
  kafka.compression.type = none
  kafka.batch.size = 16384
  kafka.linger.ms = 1
  kafka.max.request.size = 1048576
  kafka.buffer.memory = 33554432
  kafka.max.in.flight.requests.per.connection = 1
  kafka.retries = 0
  
  kafka.kerberos.enable = false
  kafka.kerberos.krb5.file = "../conf/kerberos/krb5.conf"
  kafka.kerberos.jaas.file = "../conf/kerberos/jaas.conf"
  
  ##################################################
  ######### 		    RocketMQ	     #############
  ##################################################
  rocketmq.producer.group = test
  rocketmq.enable.message.trace = false
  rocketmq.customized.trace.topic =
  rocketmq.namespace =
  rocketmq.namesrv.addr = 127.0.0.1:9876
  rocketmq.retry.times.when.send.failed = 0
  rocketmq.vip.channel.enabled = false
  rocketmq.tag = 
  
  ##################################################
  ######### 		    RabbitMQ	     #############
  ##################################################
  rabbitmq.host =
  rabbitmq.virtual.host =
  rabbitmq.exchange =
  rabbitmq.username =
  rabbitmq.password =
  rabbitmq.deliveryMode =
  ```

- 执行bin目录下的startup.sh
