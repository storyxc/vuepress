---
title: å¢é‡å¼çˆ¬è™«å®è·µæ¡ˆä¾‹ ä¸‹è½½æŒ‡å®šbç«™upä¸»çš„æ‰€æœ‰ä½œå“
date: 2021-06-19 00:24:32
permalink: /pages/940b45/
categories: 
  - python
  - Pythonçˆ¬è™«
tags: 
  - pythonçˆ¬è™«
---
# å¢é‡å¼çˆ¬è™«å®è·µæ¡ˆä¾‹ ä¸‹è½½æŒ‡å®šbç«™upä¸»çš„æ‰€æœ‰ä½œå“



## èƒŒæ™¯

å¢é‡å¼çˆ¬å–æŒ‡å®šçš„upä¸»çš„æ‰€æœ‰æŠ•ç¨¿ä½œå“ï¼Œå³å®ç°ä¸€ä¸ªå¢é‡å¼çˆ¬è™«ã€‚

è¿™æ¬¡ç¤ºèŒƒçš„upä¸»æ˜¯ä¸ªå¦¹å­ğŸ˜[kototo](https://space.bilibili.com/17485141/video)ä½¿ç”¨äº†scrapyæ¡†æ¶ï¼Œä¸»è¦æ˜¯ä¸ºäº†ç»ƒæ‰‹ï¼Œä¸ä½¿ç”¨æ¡†æ¶åè€Œä¼šæ›´ç®€å•ä¸€äº›ã€‚

pythonæ¨¡å—ï¼šscrapyã€seleniumã€requestsã€pymysql

å…¶ä»–ç¯å¢ƒï¼šffmpegã€mysql

## åˆ›å»ºä¸€ä¸ªé¡¹ç›®å¹¶åˆ›å»ºçˆ¬è™«

```bash
scrapy startproject kototo
cd kototo
scrapy genspider kototo bilibili.com
```

## çˆ¬è™«ç±»

```python
import scrapy
from selenium import webdriver
import re
import json
import requests
import os
from kototo.items import KototoItem
import pymysql


class KototoSpider(scrapy.Spider):
    name = 'kototo'
    start_urls = []

    def __init__(self):
        """
        æ„é€ å™¨,ä¸»è¦åˆå§‹åŒ–äº†seleniumå¯¹è±¡å¹¶å®ç°æ— å¤´æµè§ˆå™¨,ä»¥åŠ
        åˆå§‹åŒ–éœ€è¦çˆ¬å–çš„urlåœ°å€,å› ä¸ºbç«™çš„ç¿»é¡µæ˜¯jså®ç°çš„,æ‰€ä»¥è¦æ‰‹åŠ¨å¤„ç†ä¸€ä¸‹
        """
        super().__init__()
        # æ„é€ æ— å¤´æµè§ˆå™¨
        from selenium.webdriver.chrome.options import Options
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--disable-gpu')
        self.bro = webdriver.Chrome(chrome_options=chrome_options)
        # æŒ‡å®šçš„upä¸»çš„æŠ•ç¨¿é¡µé¢,å¯ä»¥æåˆ°å¤–é¢ä½¿ç”¨inputè¾“å…¥
        space_url = 'https://space.bilibili.com/17485141/video'
        # åˆå§‹åŒ–éœ€è¦çˆ¬å–çš„åˆ—è¡¨é¡µ
        self.init_start_urls(self.start_urls, space_url)
        # åˆ›å»ºæ¡Œé¢æ–‡ä»¶å¤¹
        self.desktop_path = os.path.join(os.path.expanduser('~'), 'Desktop\\' + self.name + '\\')
        if not os.path.exists(self.desktop_path):
            os.mkdir(self.desktop_path)

    def parse(self, response):
        """
        è§£ææ–¹æ³•,è§£æåˆ—è¡¨é¡µçš„è§†é¢‘li,æ‹¿åˆ°æ ‡é¢˜å’Œè¯¦æƒ…é¡µ,ç„¶åä¸»åŠ¨è¯·æ±‚è¯¦æƒ…é¡µ
        :param response: 
        :return: 
        """
        li_list = response.xpath('//*[@id="submit-video-list"]/ul[2]/li')
        for li in li_list:
            print(li.xpath('./a[2]/@title').extract_first())
            print(detail_url := 'https://' + li.xpath('./a[2]/@href').extract_first()[2:])
            yield scrapy.Request(url=detail_url, callback=self.parse_detail)

    def parse_detail(self, response):
        """
        å¢é‡çˆ¬å–: è§£æè¯¦æƒ…é¡µçš„éŸ³è§†é¢‘åœ°å€å¹¶äº¤ç»™ç®¡é“å¤„ç†
        ä½¿ç”¨mysqlå®ç°
        :param response: 
        :return: 
        """
        title = response.xpath('//*[@id="viewbox_report"]/h1/@title').extract_first()
        # æ›¿æ¢æ‰è§†é¢‘åç§°ä¸­æ— æ³•ç”¨åœ¨æ–‡ä»¶åä¸­æˆ–ä¼šå¯¼è‡´cmdå‘½ä»¤å‡ºé”™çš„å­—ç¬¦
        title = title.replace('-', '').replace(' ', '').replace('/', '').replace('|', '')
        play_info_list = self.get_play_info(response)
        # è¿™é‡Œä½¿ç”¨mysqlçš„å”¯ä¸€ç´¢å¼•å®ç°å¢é‡çˆ¬å–,å¦‚æœæ˜¯æœåŠ¡å™¨ä¸Šè·‘ä¹Ÿå¯ä»¥ç”¨redis
        if self.insert_info(title, play_info_list[1]):
            video_temp_path = (self.desktop_path + title + '_temp.mp4').replace('-', '')
            video_path = self.desktop_path + title + '.mp4'
            audio_path = self.desktop_path + title + '.mp3'
            item = KototoItem()
            item['video_url'] = play_info_list[0]
            item['audio_url'] = play_info_list[1]
            item['video_path'] = video_path
            item['audio_path'] = audio_path
            item['video_temp_path'] = video_temp_path
            yield item
        else:
            print(title + ': å·²ç»ä¸‹è½½è¿‡äº†!')

    def insert_info(self, vtitle, vurl):
        """
        mysqlæŒä¹…åŒ–å­˜å‚¨çˆ¬å–è¿‡çš„è§†é¢‘å†…å®¹ä¿¡æ¯
        :param vtitle: æ ‡é¢˜
        :param vurl: è§†é¢‘é“¾æ¥
        :return: 
        """
        with Mysql() as conn:
            cursor = conn.cursor(pymysql.cursors.DictCursor)
            try:
                sql = 'insert into tb_kototo(title,url) values("%s","%s")' % (vtitle, vurl)
                res = cursor.execute(sql)
                conn.commit()
                if res == 1:
                    return True
                else:
                    return False
            except:
                return False

    def get_play_info(self, resp):
        """
        è§£æè¯¦æƒ…é¡µçš„æºä»£ç ,æå–å…¶ä¸­çš„è§†é¢‘å’Œæ–‡ä»¶çœŸå®åœ°å€
        :param resp: 
        :return: 
        """
        json_data = json.loads(re.findall('<script>window\.__playinfo__=(.*?)</script>', resp.text)[0])
        # æ‹¿åˆ°è§†é¢‘å’ŒéŸ³é¢‘çš„çœŸå®é“¾æ¥åœ°å€
        video_url = json_data['data']['dash']['video'][0]['backupUrl'][0]
        audio_url = json_data['data']['dash']['audio'][0]['backupUrl'][0]
        return video_url, audio_url

    def init_start_urls(self, url_list, person_page):
        """
        åˆå§‹åŒ–éœ€è¦çˆ¬å–çš„åˆ—è¡¨é¡µ,ç”±äºbç«™ä½¿ç”¨jsç¿»é¡µ,æ— æ³•åœ¨æºç ä¸­æ‰¾åˆ°ç¿»é¡µåœ°å€,
        éœ€è¦è‡ªå·±æ‰‹åŠ¨å®ç°è§£æç¿»é¡µurlçš„æ“ä½œ
        :param url_list: 
        :param person_page: 
        :return: 
        """
        mid = re.findall('https://space.bilibili.com/(.*?)/video\w*', person_page)[0]
        url = 'https://api.bilibili.com/x/space/arc/search?mid=' + mid + '&ps=30&tid=0&pn=1&keyword=&order=pubdate&jsonp=jsonp'
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36',
            'Referer': 'https://www.bilibili.com'
        }
        json_data = requests.get(url=url, headers=headers).json()
        total_count = json_data['data']['page']['count']
        page_size = json_data['data']['page']['ps']
        if total_count <= page_size:
            page_count = 1
        elif total_count % page_size == 0:
            page_count = total_count / page_size
        else:
            page_count = total_count // page_size + 1

        url_template = 'https://space.bilibili.com/' + mid + '/video?tid=0&page=' + '%d' + '&keyword=&order=pubdate'
        for i in range(page_count):
            page_no = i + 1
            url_list.append(url_template % page_no)

    def closed(self, spider):
        """
        çˆ¬è™«ç»“æŸå…³é—­seleniumçª—å£
        :param spider: 
        :return: 
        """
        self.bro.quit()


class Mysql(object):
    def __enter__(self):
        self.connection = pymysql.connect(host='127.0.0.1', port=3306, user='root', password='root', database='python')
        return self.connection

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.connection.close()

```

## ä¸‹è½½ä¸­é—´ä»¶

```python
# Define here the models for your spider middleware
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/spider-middleware.html

from scrapy import signals

# useful for handling different item types with a single interface
from itemadapter import is_item, ItemAdapter


class KototoSpiderMiddleware:
    # Not all methods need to be defined. If a method is not defined,
    # scrapy acts as if the spider middleware does not modify the
    # passed objects.

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_spider_input(self, response, spider):
        # Called for each response that goes through the spider
        # middleware and into the spider.

        # Should return None or raise an exception.
        return None

    def process_spider_output(self, response, result, spider):
        # Called with the results returned from the Spider, after
        # it has processed the response.

        # Must return an iterable of Request, or item objects.
        for i in result:
            yield i

    def process_spider_exception(self, response, exception, spider):
        # Called when a spider or process_spider_input() method
        # (from other spider middleware) raises an exception.

        # Should return either None or an iterable of Request or item objects.
        pass

    def process_start_requests(self, start_requests, spider):
        # Called with the start requests of the spider, and works
        # similarly to the process_spider_output() method, except
        # that it doesnâ€™t have a response associated.

        # Must return only requests (not items).
        for r in start_requests:
            yield r

    def spider_opened(self, spider):
        spider.logger.info('Spider opened: %s' % spider.name)


class KototoDownloaderMiddleware:

    def process_request(self, request, spider):
        return None

    def process_response(self, request, response, spider):
        """
        ç¯¡æ”¹åˆ—è¡¨é¡µçš„å“åº”æ•°æ®:
            è§†é¢‘åˆ—è¡¨æ˜¯é€šè¿‡ajaxè¯·æ±‚åŠ¨æ€åŠ è½½çš„,å› æ­¤è¦é€šè¿‡seleniumå»åŠ è½½è¿™éƒ¨åˆ†æ•°æ®
            å¹¶ç¯¡æ”¹å“åº”å†…å®¹
        :param request: 
        :param response: 
        :param spider: 
        :return: 
        """
        urls = spider.start_urls
        bro = spider.bro
        from scrapy.http import HtmlResponse
        from time import sleep
        if request.url in urls:
            """
            å¦‚æœæ˜¯åˆ—è¡¨é¡µå°±è¿›è¡Œå“åº”ç¯¡æ”¹æ“ä½œ
            """
            bro.get(request.url)
            sleep(3)
            page_data = bro.page_source
            new_response = HtmlResponse(url=request.url, body=page_data, encoding='utf-8', request=request)
            # è¿”å›ç¯¡æ”¹è¿‡çš„å“åº”å¯¹è±¡
            return new_response
        return response

    def process_exception(self, request, exception, spider):
        pass
```

## Item

```python
import scrapy


class KototoItem(scrapy.Item):
    video_path = scrapy.Field()
    video_url = scrapy.Field()
    audio_path = scrapy.Field()
    audio_url = scrapy.Field()
    video_temp_path = scrapy.Field()
```

## Pipeline

```python
import requests
import os

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.93 Safari/537.36',
    'Referer': 'https://www.bilibili.com'
}


class KototoPipeline(object):
    def process_item(self, item, spider):
        video = item['video_url']
        audio = item['audio_url']
        video_temp_path = item['video_temp_path']
        audio_path = item['audio_path']
        video_data = requests.get(url=video, headers=headers).content
        audio_data = requests.get(url=audio, headers=headers).content
        with open(video_temp_path, 'wb') as f:
            f.write(video_data)
        with open(audio_path, 'wb') as f:
            f.write(audio_data)
        return item


class MergePipeline(object):
    """
    åˆ é™¤ä¸´æ—¶æ–‡ä»¶
    """

    def process_item(self, item, spider):
        video_temp_path = item['video_temp_path']
        audio_path = item['audio_path']
        video_path = item['video_path']
        cmd = 'ffmpeg -y -i ' + video_temp_path + ' -i ' \
              + audio_path + ' -c:v copy -c:a aac -strict experimental ' + video_path
        print(cmd)
        # subprocess.Popen(cmd, shell=True)
        os.system(cmd)
        os.remove(video_temp_path)
        os.remove(audio_path)
        print(video_path, 'ä¸‹è½½å®Œæˆ')
        return item

```

## settings

```python
BOT_NAME = 'kototo'

SPIDER_MODULES = ['kototo.spiders']
NEWSPIDER_MODULE = 'kototo.spiders'


USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36'


ROBOTSTXT_OBEY = False


DEFAULT_REQUEST_HEADERS = {
  'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
  'Referer': 'https://space.bilibili.com/17485141/video',
  'Origin':  'https://space.bilibili.com'
}
FILES_STORE = './files'
DOWNLOADER_MIDDLEWARES = {
   'kototo.middlewares.KototoDownloaderMiddleware': 543,
}

ITEM_PIPELINES = {
    # ä¸‹è½½
   'kototo.pipelines.KototoPipeline': 1,
   # åˆå¹¶
    'kototo.pipelines.MergePipeline': 2,
}

```



## å¯åŠ¨

- å‘½ä»¤å¯åŠ¨ï¼š`scrapy crawl kototo`

- é…ç½®pycharmå¯åŠ¨ï¼ˆæ¨èï¼‰

  ![image-20210503002914982](https://io.storyxc.com/image-20210503002914982.png)

### ä¸‹è½½ç»“æœ

![image-20210503003351357](https://io.storyxc.com/image-20210503003351357.png)

### mysql

![image-20210503003551035](https://io.storyxc.com/image-20210503003551035.png)

## å†æ¬¡å°è¯•ä¸‹è½½æ—¶

![image-20210503003617326](https://io.storyxc.com/image-20210503003617326.png)



å·²ç»çˆ¬å–è¿‡çš„èµ„æºä¼šæç¤ºå·²ç»ä¸‹è½½è¿‡ï¼Œåªä¼šå¤„ç†æ›´æ–°çš„å†…å®¹ã€‚

